{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p36",
      "language": "python",
      "name": "conda_pytorch_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HMUNACHI/Sentiment-Analyser/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e55RW8bSQkC"
      },
      "source": [
        "Using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "\n",
        "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6aa1y29SQkE",
        "outputId": "f2aca5ab-3c91-454b-a5e7-8461256e0b7f"
      },
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘../data’: File exists\n",
            "--2020-06-13 21:40:42--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  23.1MB/s    in 4.6s    \n",
            "\n",
            "2020-06-13 21:40:46 (17.5 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Maa4b30SQkH"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "                    \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "                \n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8sPIO45SQkI",
        "outputId": "bed96181-8691-471f-962c-7d993f0cc3c3"
      },
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPcstFm2SQkJ"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIljY_uwSQkK",
        "outputId": "834a803a-d4b7-4ac9-84af-4553efe51ffa"
      },
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAeMXjZ7SQkL",
        "outputId": "11ddbf31-6e9d-477f-a92d-9d60027afc30"
      },
      "source": [
        "print(train_X[100])\n",
        "print(train_y[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full House is a wonderful sitcom that is about a dad, Danny Tanner, whose wife had just died in a car crash. So Danny asks his brother in law, Jesse Katsopolis, and best friend, Joey Gladstone, to help raise his three girls, Donna Jo 'DJ' Tanner, Stephanie Tanner, and Michelle Tanner. This is my favorite show ever, and I can watch it all day long. And something on Full House is always making me laugh and there are sad parts also. There is never a dull moment in Full House. The main characters are played by, Bob Saget(Danny Tanner), John Stamos(Jesse Katsopolis), Dave Coulier(Joey Gladstone), Candace Cameron(DJ Tanner), Jodie Sweetin(Stephanie Tanner), and Mary-Kate & Ashley Olsen(Michelle Tanner).\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF6ryod-SQkN"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3_kVHBpSQkN"
      },
      "source": [
        "The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. As a check to ensure we know how everything is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6EWepLkSQkQ",
        "outputId": "d3266000-50e2-4fe2-f96f-ac4d669e5938"
      },
      "source": [
        "#Apply review_to_words to a review (train_X[100] or any other review)\n",
        "review_to_words(train_X[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['full',\n",
              " 'hous',\n",
              " 'wonder',\n",
              " 'sitcom',\n",
              " 'dad',\n",
              " 'danni',\n",
              " 'tanner',\n",
              " 'whose',\n",
              " 'wife',\n",
              " 'die',\n",
              " 'car',\n",
              " 'crash',\n",
              " 'danni',\n",
              " 'ask',\n",
              " 'brother',\n",
              " 'law',\n",
              " 'jess',\n",
              " 'katsopoli',\n",
              " 'best',\n",
              " 'friend',\n",
              " 'joey',\n",
              " 'gladston',\n",
              " 'help',\n",
              " 'rais',\n",
              " 'three',\n",
              " 'girl',\n",
              " 'donna',\n",
              " 'jo',\n",
              " 'dj',\n",
              " 'tanner',\n",
              " 'stephani',\n",
              " 'tanner',\n",
              " 'michel',\n",
              " 'tanner',\n",
              " 'favorit',\n",
              " 'show',\n",
              " 'ever',\n",
              " 'watch',\n",
              " 'day',\n",
              " 'long',\n",
              " 'someth',\n",
              " 'full',\n",
              " 'hous',\n",
              " 'alway',\n",
              " 'make',\n",
              " 'laugh',\n",
              " 'sad',\n",
              " 'part',\n",
              " 'also',\n",
              " 'never',\n",
              " 'dull',\n",
              " 'moment',\n",
              " 'full',\n",
              " 'hous',\n",
              " 'main',\n",
              " 'charact',\n",
              " 'play',\n",
              " 'bob',\n",
              " 'saget',\n",
              " 'danni',\n",
              " 'tanner',\n",
              " 'john',\n",
              " 'stamo',\n",
              " 'jess',\n",
              " 'katsopoli',\n",
              " 'dave',\n",
              " 'coulier',\n",
              " 'joey',\n",
              " 'gladston',\n",
              " 'candac',\n",
              " 'cameron',\n",
              " 'dj',\n",
              " 'tanner',\n",
              " 'jodi',\n",
              " 'sweetin',\n",
              " 'stephani',\n",
              " 'tanner',\n",
              " 'mari',\n",
              " 'kate',\n",
              " 'ashley',\n",
              " 'olsen',\n",
              " 'michel',\n",
              " 'tanner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOe7ExJTSQkS"
      },
      "source": [
        "The method below applies the `review_to_words` method to each of the reviews in the training and testing datasets. In addition it caches the results. This is because performing this processing step can take a long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joH8oPaQSQkS"
      },
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF2BcYoJSQkT",
        "outputId": "8257df4e-c876-4f87-d2d5-5c25f0caaa77"
      },
      "source": [
        "# Preprocess data\n",
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx2roIrCSQkd"
      },
      "source": [
        "import pandas as pd\n",
        "    \n",
        "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
        "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hb-tuWISQke",
        "outputId": "bb0b7007-07e6-490d-e85c-8621a32c11ff"
      },
      "source": [
        "!pygmentize train/model.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
            "\r\n",
            "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
            "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
            "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
            "\r\n",
            "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
            "        \r\n",
            "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[36mNone\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        x = x.t()\r\n",
            "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
            "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
            "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
            "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
            "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
            "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
            "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqmXrveMSQkf"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Read in only the first 250 rows\n",
        "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
        "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
        "# Build the dataloader\n",
        "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc8zwvBnSQki",
        "outputId": "6da1e893-5fa6-4ac8-eac1-0e2463eae6d2"
      },
      "source": [
        "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WBPActySQki"
      },
      "source": [
        "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "som8Ne5KSQkj"
      },
      "source": [
        "# split the data into chunks and send each chunk seperately, accumulating the results.\n",
        "\n",
        "def predict(data, rows=512):\n",
        "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
        "    predictions = np.array([])\n",
        "    for array in split_array:\n",
        "        predictions = np.append(predictions, predictor.predict(array))\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7E0uLQ5SQkj"
      },
      "source": [
        "predictions = predict(test_X.values)\n",
        "predictions = [round(num) for num in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEU9KwHzSQkj",
        "outputId": "854890f5-c364-404d-abdb-da8348bdc2d9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_y, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.85596"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6oJokbMSQkm"
      },
      "source": [
        "!pygmentize serve/predict.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM5wWMnySQkq",
        "outputId": "cc55c0dc-4a83-4a60-8fb1-1eb219aa4300"
      },
      "source": [
        "predictor.endpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sagemaker-pytorch-2020-06-13-22-04-44-028'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4QT1NyzSQkt"
      },
      "source": [
        "predictor.delete_endpoint()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}